{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aspect_level_sentiment_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnRUuCMOPh1b",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we exploit document knowledge for aspect-level sentiment classification. More specifically, we build an attention-based aspect-level sentiment classification model with biLISTM. The biLSTM network learns sentence representations from input sequences. Additionally, an attention network assigns an attention score over a sequence of biLSTM hidden states based on aspect term representations. Then, a fully connected network predicts the sentiment label.\n",
        "\n",
        "The model is trained based on transfer learning, i.e., we first train the parameters of a model on document-level examples, and we use the learned parameters to initialize and fine-tune the parameters of the aspect-level model.\n",
        "\n",
        "This notebook is based on the work proposed by He et al. in [1].\n",
        "\n",
        "[1] R. He, WS. Lee & D. Dahlmeier. Exploiting document knowledge for aspect-level sentiment classification. 2018. https://arxiv.org/abs/1806.04346."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVSpV6k0QhJr",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FB6yxc4BM9uU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "cbbb71ed-1848-4fc3-8409-3b6485d1f968"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BPDMoaQNNAC5",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import operator\n",
        "import numpy as np\n",
        "import re\n",
        "import _pickle as cPickle\n",
        "\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k9TH6Um2s-7d"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LbWy_h2KtNgR",
        "colab": {}
      },
      "source": [
        "def read_pickle(data_path, file_name):\n",
        "\n",
        "    f = open(os.path.join(data_path, file_name), 'rb')\n",
        "    read_file = cPickle.load(f)\n",
        "    f.close()\n",
        "\n",
        "    return read_file\n",
        "\n",
        "\n",
        "def save_pickle(data_path, file_name, data):\n",
        "\n",
        "    f = open(os.path.join(data_path, file_name), 'wb')\n",
        "    cPickle.dump(data, f)\n",
        "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fuAMxdYBkv_x",
        "colab": {}
      },
      "source": [
        "# Path pointing to the aspect-level data\n",
        "aspect_path = '/drive/My Drive/projects/aspect-level-data' \n",
        "\n",
        "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
        "\n",
        "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
        "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
        "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
        "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
        "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
        "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
        "\n",
        "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
        "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
        "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')\n",
        "\n",
        "pretrain_data = read_pickle(aspect_path, 'pretrain_data.pkl')\n",
        "pretrain_label = read_pickle(aspect_path, 'pretrain_label.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rWVgGO8RlVIJ",
        "colab": {}
      },
      "source": [
        "class Dataiterator_doc():\n",
        "    '''\n",
        "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
        "      2) Access to the entire dataset using all()\n",
        "    '''\n",
        "    def __init__(self, X, y, seq_length=32, decoder_dim=300, batch_size=32):      \n",
        "        self.X = X \n",
        "        self.y = y \n",
        "        self.num_data = len(X) # total number of examples\n",
        "        self.batch_size = batch_size # batch size\n",
        "        self.reset() # initial: shuffling examples and set index to 0\n",
        "    \n",
        "\n",
        "    def __iter__(self): # iterates data\n",
        "        return self\n",
        "\n",
        "\n",
        "    def reset(self): # initials\n",
        "        self.idx = 0\n",
        "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
        "        \n",
        "\n",
        "    def __next__(self): # return model inputs - outputs per batch\n",
        "        X_ids = [] # hold ids per batch \n",
        "        while len(X_ids) < self.batch_size:\n",
        "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
        "            X_ids.append(X_id)\n",
        "            self.idx += 1 # \n",
        "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
        "                self.reset()\n",
        "                raise StopIteration()\n",
        "                \n",
        "        batch_X = self.X[np.array(X_ids)] # X values (encoder input) per batch\n",
        "        batch_y = self.y[np.array(X_ids)] # y_in values (decoder input) per batch\n",
        "        return batch_X, batch_y\n",
        "\n",
        "          \n",
        "    def all(self): # return all data examples\n",
        "        return self.X, self.y\n",
        "\n",
        "\n",
        "class Dataiterator_aspect():\n",
        "    '''\n",
        "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
        "      2) Access to the entire dataset using all()\n",
        "    '''\n",
        "    def __init__(self, aspect_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
        "        \n",
        "        len_aspect_data = len(aspect_data[0])\n",
        "        #self.len_doc_data = len(doc_data[0])\n",
        "        \n",
        "        self.X_aspect = aspect_data[0] \n",
        "        self.y_aspect = aspect_data[1]\n",
        "        self.aspect_terms = aspect_data[2]  \n",
        "        self.num_data = len_aspect_data\n",
        "        self.batch_size = batch_size # batch size\n",
        "        self.reset() # initial: shuffling examples and set index to 0\n",
        "    \n",
        "    def __iter__(self): # iterates data\n",
        "        return self\n",
        "\n",
        "\n",
        "    def reset(self): # initials\n",
        "        self.idx = 0\n",
        "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
        "        \n",
        "\n",
        "    def __next__(self): # return model inputs - outputs per batch\n",
        "        \n",
        "        X_ids = [] # hold ids per batch \n",
        "        while len(X_ids) < self.batch_size:\n",
        "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
        "            X_ids.append(X_id)\n",
        "            self.idx += 1 # \n",
        "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
        "                self.reset()\n",
        "                raise StopIteration()\n",
        "                \n",
        "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
        "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
        "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
        "        \n",
        "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms\n",
        "\n",
        "          \n",
        "    def all(self): # return all data examples\n",
        "        return self.X_aspect, self.y_aspect, self.aspect_terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38IEEx0du3vW",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM,Bidirectional\n",
        "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "\n",
        "import keras.backend as K\n",
        "import keras.optimizers as opt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cduHSpsSnVue",
        "colab": {}
      },
      "source": [
        "overal_maxlen = 82\n",
        "overal_maxlen_aspect = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bhTQke3HnvHN"
      },
      "source": [
        "\n",
        "#Defining the Attention Network Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnDX-po3_50B",
        "colab": {}
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self,  **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Content Attention mechanism.\n",
        "        Supports Masking.\n",
        "        \"\"\"\n",
        "       \n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert type(input_shape) == list\n",
        "       \n",
        "        self.steps = input_shape[0][1]\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[0][-1], input_shape[1][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "    def compute_mask(self, input_tensor, mask=None):\n",
        "        assert type(input_tensor) == list\n",
        "        assert type(mask) == list\n",
        "        return None\n",
        "\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        x = input_tensor[0] # h matrix\n",
        "        aspect = input_tensor[1] # t vector\n",
        "        mask = mask[0]\n",
        "        \n",
        "        t = aspect\n",
        "        h = x\n",
        "        \n",
        "        # h is a sequence of vectors h_1, h_2, ..., h_LSTM_dim\n",
        "        # each h_1 is of dimension steps\n",
        "        # So we have\n",
        "        # h_11 h_12 .... h_1steps\n",
        "        # h_21 h_22 .... h_2steps\n",
        "        # .... .... .... ....\n",
        "        # h_LSTM_dim1 .. h_LSTM_dimsteps\n",
        "        # Shape is (batch_size, LSTM_dim, steps)\n",
        "\n",
        "        # W_a is the weights matrix\n",
        "        # It is of shape (LSTM_dim, embedding_dim)\n",
        "        # A difficult thing to overcome is that W_a has no batch_size axis.\n",
        "\n",
        "        # t is the already averaged embeddings x_1, ..., x_m.\n",
        "        # It is of the form t_1, t_2, ..., t_embedding_dim, \n",
        "        # And of shape (batch_size, embedding_dim)\n",
        "\n",
        "        # We first compute the dot product of W and t \n",
        "        # t is assumed to be a column vector, so we need to transpose it\n",
        "        # We also need to transpose W since the batch_size axis is at the back\n",
        "        # We need to have that in the front\n",
        "        Wat = K.transpose(K.dot(self.W, K.transpose(t)))\n",
        "        # Wat is of shape (batch_size, LSTM_dim)\n",
        "\n",
        "        # We have for h still the same: (batch_size, LSTM_dim, steps)\n",
        "        # We now do multiply the h_matrix with the Wat column for each\n",
        "        # element in the batch. Wat column dim must be h row dim. \n",
        "        # so we transpose h, with a batch transpose to keep batch axis at 0.\n",
        "        h_tran = K.permute_dimensions(h, (0, 2, 1))\n",
        "\n",
        "        # So we now matrix multiply shapes: \n",
        "        # (batch_size, LSTM_dim, steps) * (batch_size, LSTM_dim)\n",
        "        # Using the convenient batch_dot, this yields (batch_size, steps)\n",
        "        hTWat = K.batch_dot(Wat, h_tran)\n",
        "\n",
        "        # We can now compute the aspect vector\n",
        "        a = K.softmax(K.tanh(hTWat))\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        return a\n",
        "\n",
        "   \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5PSPx85EiVn",
        "colab": {}
      },
      "source": [
        "class Average(Layer):  \n",
        "    def __init__(self, mask_zero=True, **kwargs):\n",
        "        self.mask_zero = mask_zero\n",
        "        self.supports_masking = True\n",
        "        super(Average, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x,mask=None):\n",
        "        if self.mask_zero:           \n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            mask = K.expand_dims(mask)\n",
        "            x = x * mask\n",
        "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
        "        else:\n",
        "            return K.mean(x, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "    \n",
        "    def compute_mask(self, x, mask):\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsZtx2PbEqoh"
      },
      "source": [
        "#Establishing the computation graph for the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xe55OCNZEmYY",
        "colab": {}
      },
      "source": [
        "dropout = 0.1   \n",
        "recurrent_dropout = 0.4\n",
        "vocab_size = len(vocab)\n",
        "num_outputs = 3  # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2BpzACdBp3xG"
      },
      "source": [
        "##Input tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOi3CcOxE1MG",
        "colab": {}
      },
      "source": [
        "# Inputs\n",
        "sentence_input = Input(shape=(overal_maxlen,), dtype='int32', name='sentence_input')\n",
        "aspect_input = Input(shape=(overal_maxlen_aspect,), dtype='int32', name='aspect_input')\n",
        "pretrain_input = Input(shape=(None,), dtype='int32', name='pretrain_input')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_vQ0z8KmrL3_"
      },
      "source": [
        "##Shared WordEmbedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GFEhEt9EE4Sn",
        "colab": {}
      },
      "source": [
        "# Represent aspect as averaged word embedding\n",
        "word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')\n",
        "aspect_term_embs = word_emb(aspect_input)\n",
        "aspect_embs = Average(mask_zero=True, name='aspect_emb')(aspect_term_embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8gGd94OGE-Gr",
        "colab": {}
      },
      "source": [
        "# Sentence representation from embedding\n",
        "sentence_embs = word_emb(sentence_input)\n",
        "pretrain_embs = word_emb(pretrain_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pmcnAQufrc7o"
      },
      "source": [
        "##Shared BiLSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bm_yCjX_F7ml",
        "colab": {}
      },
      "source": [
        "# Sentence representation from embedding\n",
        "bilstm = Bidirectional(LSTM(300, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, name='lstm'), name='bilstm')\n",
        "sentence_bilstm = bilstm(sentence_embs)\n",
        "pretrain_bilstm = bilstm(pretrain_embs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "99ZNrbkmrllN"
      },
      "source": [
        "##Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HO5Pj6QANz7U",
        "colab": {}
      },
      "source": [
        "att_weights = Attention(name='att_weights')([sentence_bilstm, aspect_embs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5-LY6jF8r3mO"
      },
      "source": [
        "##Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nxrmw-Grr6pK",
        "colab": {}
      },
      "source": [
        "pretrain_output = Average(mask_zero=True)(pretrain_bilstm)\n",
        "\n",
        "# Function for computed the weighted sum\n",
        "def weighted_sum(tensor):\n",
        "  x = tensor[0]\n",
        "  a = tensor[1]\n",
        "  a = K.expand_dims(a, axis=2)\n",
        "  return K.sum(x * a, axis=1)\n",
        "\n",
        "# Get the weighted sum of att_weights and sentence_bilstm\n",
        "weighted_sum_layer = Lambda(weighted_sum)([sentence_bilstm, att_weights])\n",
        "\n",
        "# Played around with dropout, first few test runs did not seem to improve the accuracies\n",
        "# weighted_sum_layer = Dropout(0.4)(weighted_sum_layer)\n",
        "# pretrain_output = Dropout(0.4)(pretrain_output)\n",
        "\n",
        "sentence_output = Dense(num_outputs, name='dense_1')(weighted_sum_layer)\n",
        "pretrain_output = Dense(num_outputs, name='dense_2')(pretrain_output)\n",
        "\n",
        "aspect_probs = Activation('softmax', name='aspect_model')(sentence_output)\n",
        "doc_probs = Activation('softmax', name='pretrain_model')(pretrain_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0XLv1t9Ou3vx"
      },
      "source": [
        "#Build Models for document-level and aspect-level data\n",
        "- The two models shared the embedding, BiLSTM, Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RX8qKfNWu3v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "959ff97f-5fd5-414d-930d-37e4d19b5e42"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "model1 = Model(inputs=pretrain_input, outputs=doc_probs)\n",
        "model2 = Model(inputs=[sentence_input, aspect_input], outputs=[aspect_probs])\n",
        "\n",
        "# plot_model(model1, show_shapes=True)\n",
        "# plot_model(model2, show_shapes=True)\n",
        "model1.summary()\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pretrain_input (InputLayer)  (None, None)              0         \n",
            "_________________________________________________________________\n",
            "word_emb (Embedding)         multiple                  3000900   \n",
            "_________________________________________________________________\n",
            "bilstm (Bidirectional)       multiple                  1442400   \n",
            "_________________________________________________________________\n",
            "average_2 (Average)          (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 1803      \n",
            "_________________________________________________________________\n",
            "pretrain_model (Activation)  (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 4,445,103\n",
            "Trainable params: 4,445,103\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "sentence_input (InputLayer)     (None, 82)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "aspect_input (InputLayer)       (None, 7)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_emb (Embedding)            multiple             3000900     aspect_input[0][0]               \n",
            "                                                                 sentence_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bilstm (Bidirectional)          multiple             1442400     word_emb[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "aspect_emb (Average)            (None, 300)          0           word_emb[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "att_weights (Attention)         (None, 82)           180000      bilstm[0][0]                     \n",
            "                                                                 aspect_emb[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 600)          0           bilstm[0][0]                     \n",
            "                                                                 att_weights[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            1803        lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "aspect_model (Activation)       (None, 3)            0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,625,103\n",
            "Trainable params: 4,625,103\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FWqofwqBsgsn"
      },
      "source": [
        "\n",
        "#Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WSLYsZm7yPwi"
      },
      "source": [
        "##Train on document-level data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJmrVe3zwdlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_generator_doc(model, batch_train_iter, batch_val_iter):\n",
        "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
        "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
        "                                     monitor='val_loss', save_best_only=False, \\\n",
        "                                     save_weights_only=True)\n",
        "                     ]\n",
        "\n",
        "    def train_gen():\n",
        "        while True:\n",
        "            train_batches = [[X, y] for X, y in batch_train_iter]\n",
        "            for train_batch in train_batches:\n",
        "                yield train_batch\n",
        "                \n",
        "    def val_gen():\n",
        "        while True:\n",
        "            val_batches = [[X, y] for X, y in batch_val_iter]\n",
        "            for val_batch in val_batches:\n",
        "                yield val_batch\n",
        "                \n",
        "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
        "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
        "                                  epochs=20, callbacks=earlystop_callbacks)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur7SyJtIu8Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_generator_aspect(model, batch_train_iter, batch_val_iter):\n",
        "    \n",
        "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
        "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
        "                                     monitor='val_loss', save_best_only=False, \\\n",
        "                                     save_weights_only=True)\n",
        "                     ]\n",
        "    \n",
        "    def train_gen():\n",
        "        while True:\n",
        "            train_batches = [[[X, aspect], [y]] for X, y, \\\n",
        "                             aspect in batch_train_iter]\n",
        "            for train_batch in train_batches:\n",
        "                yield train_batch\n",
        "                \n",
        "    def val_gen():\n",
        "        while True:\n",
        "            val_batches = [[[X, aspect], [y]] for X, y, \\\n",
        "                           aspect in batch_val_iter]\n",
        "            for val_batch in val_batches:\n",
        "                yield val_batch\n",
        "                \n",
        "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
        "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
        "                                  epochs=20, callbacks=earlystop_callbacks)\n",
        "    print(history.history['val_loss'], history.history['val_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXeMGd_2D4UY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.optimizers as opt\n",
        "\n",
        "optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n",
        "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "train_steps_epoch = len(pretrain_data)/batch_size\n",
        "batch_train_iter_doc = Dataiterator_doc(pretrain_data, pretrain_label, batch_size)\n",
        "\n",
        "batch_val_iter_doc = Dataiterator_doc(dev_x, dev_y, batch_size)\n",
        "val_steps_epoch = len(dev_x)/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dYVTUd2D7La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "outputId": "29528ed6-97c5-4898-97ee-38421cb8c99d"
      },
      "source": [
        "history = train_generator_doc(model1, batch_train_iter_doc, batch_val_iter_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "235/234 [==============================] - 329s 1s/step - loss: 0.9384 - categorical_accuracy: 0.5593 - val_loss: 0.9180 - val_categorical_accuracy: 0.6172\n",
            "Epoch 2/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.7754 - categorical_accuracy: 0.6489 - val_loss: 0.7739 - val_categorical_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.7516 - categorical_accuracy: 0.6578 - val_loss: 0.9054 - val_categorical_accuracy: 0.4766\n",
            "Epoch 4/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.7165 - categorical_accuracy: 0.6818 - val_loss: 1.1347 - val_categorical_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.6589 - categorical_accuracy: 0.7188 - val_loss: 0.6794 - val_categorical_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "235/234 [==============================] - 326s 1s/step - loss: 0.6549 - categorical_accuracy: 0.7162 - val_loss: 1.2245 - val_categorical_accuracy: 0.4375\n",
            "Epoch 7/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.6716 - categorical_accuracy: 0.7104 - val_loss: 1.0778 - val_categorical_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "235/234 [==============================] - 327s 1s/step - loss: 0.6482 - categorical_accuracy: 0.7182 - val_loss: 0.8870 - val_categorical_accuracy: 0.5156\n",
            "Epoch 9/20\n",
            "235/234 [==============================] - 326s 1s/step - loss: 0.6100 - categorical_accuracy: 0.7428 - val_loss: 1.3435 - val_categorical_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.6176 - categorical_accuracy: 0.7351 - val_loss: 1.3184 - val_categorical_accuracy: 0.4922\n",
            "Epoch 11/20\n",
            "235/234 [==============================] - 326s 1s/step - loss: 0.6228 - categorical_accuracy: 0.7282 - val_loss: 1.6611 - val_categorical_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.6141 - categorical_accuracy: 0.7407 - val_loss: 1.2933 - val_categorical_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "235/234 [==============================] - 326s 1s/step - loss: 0.5698 - categorical_accuracy: 0.7624 - val_loss: 0.6710 - val_categorical_accuracy: 0.5625\n",
            "Epoch 14/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.5704 - categorical_accuracy: 0.7606 - val_loss: 1.3148 - val_categorical_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.5777 - categorical_accuracy: 0.7565 - val_loss: 1.3501 - val_categorical_accuracy: 0.4844\n",
            "Epoch 16/20\n",
            "235/234 [==============================] - 326s 1s/step - loss: 0.5961 - categorical_accuracy: 0.7475 - val_loss: 1.5321 - val_categorical_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "235/234 [==============================] - 325s 1s/step - loss: 0.5271 - categorical_accuracy: 0.7858 - val_loss: 1.3106 - val_categorical_accuracy: 0.4844\n",
            "Epoch 18/20\n",
            "235/234 [==============================] - 327s 1s/step - loss: 0.5454 - categorical_accuracy: 0.7730 - val_loss: 1.7001 - val_categorical_accuracy: 0.4453\n",
            "Epoch 19/20\n",
            "235/234 [==============================] - 327s 1s/step - loss: 0.5563 - categorical_accuracy: 0.7688 - val_loss: 1.1743 - val_categorical_accuracy: 0.5547\n",
            "Epoch 20/20\n",
            "235/234 [==============================] - 327s 1s/step - loss: 0.5520 - categorical_accuracy: 0.7636 - val_loss: 1.1191 - val_categorical_accuracy: 0.5469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L-Aeo4MxPi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/drive/My Drive/projects/saved-weights'\n",
        "save_weights = False\n",
        "\n",
        "if save_weights:\n",
        "  # Get weights of trained model1\n",
        "  # The shared weights are the weights of the embedding layer,\n",
        "  # the bidirectional LSTM layer, and the prediction layer\n",
        "  embedding_weights = model1.get_layer('word_emb').get_weights()[0]\n",
        "  bilstm_weights = model1.get_layer('bilstm').get_weights()\n",
        "  dense_2_weights = model1.get_layer('dense_2').get_weights() \n",
        "\n",
        "  # Function to save weights to a name\n",
        "  def save_weights(weights, save_name):\n",
        "      with open(save_path + save_name + '.pkl', 'wb') as f:\n",
        "          cPickle.dump(weights, f)\n",
        "\n",
        "  save_weights(embedding_weights, 'embedding_weights')\n",
        "  save_weights(bilstm_weights, 'bilstm_weights')\n",
        "  save_weights(dense_2_weights, 'dense_2_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w-6w2Rs7Vui",
        "colab_type": "text"
      },
      "source": [
        "For convenience, we save the weights of the shared layer for transfer learning.\n",
        "The reason for this is that we do not have to retrain model1 to get the weights for model2. We also initialise the weights for model2 to these pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGeFllih7U3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_path = '/drive/My Drive/projects/saved-weights'\n",
        "load_pretrained_weights = True\n",
        "\n",
        "if load_pretrained_weights:\n",
        "  # Retrieve the picke files of pretrained weights\n",
        "  pretrained_embedding_weights = open(load_path + 'embedding_weights.pkl', 'rb')\n",
        "  pretrained_bilstm_weights = open(load_path + 'bilstm_weights.pkl', 'rb')\n",
        "  pretrained_dense_2_weights = open(load_path + 'dense_2_weights.pkl', 'rb')\n",
        "\n",
        "  # Load the weights from the pickle files\n",
        "  loaded_embedding_weights = cPickle.load(pretrained_embedding_weights)\n",
        "  loaded_bilstm_weights = cPickle.load(pretrained_bilstm_weights)\n",
        "  loaded_dense_2_weights = cPickle.load(pretrained_dense_2_weights)\n",
        "\n",
        "  # Initialise weights of model2 with these weights\n",
        "  model2.get_layer('word_emb').set_weights(K.expand_dims(loaded_embedding_weights, axis=0))\n",
        "  model2.get_layer('bilstm').set_weights(loaded_bilstm_weights)\n",
        "  model2.get_layer('dense_1').set_weights(loaded_dense_2_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-SYaHNMyXWX"
      },
      "source": [
        "##Train on aspect-level data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lCLGP6HTFsKL",
        "colab": {}
      },
      "source": [
        "model2 = Model(inputs=[sentence_input, aspect_input], outputs=[aspect_probs])\n",
        "\n",
        "batch_size = 32\n",
        "train_steps_epoch = len(train_x)/batch_size\n",
        "batch_train_iter_aspect = Dataiterator_aspect([train_x, train_y, train_aspect], batch_size)\n",
        "val_steps_epoch = len(dev_x)/batch_size\n",
        "batch_val_iter_aspect = Dataiterator_aspect([dev_x, dev_y, dev_aspect], batch_size)\n",
        "\n",
        "optimizer = opt.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
        "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkeMV9NAofaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "7b2b07bd-43cb-4f20-934d-1f1993489411"
      },
      "source": [
        "train_generator_aspect(model2, batch_train_iter_aspect, batch_val_iter_aspect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "58/57 [==============================] - 25s 436ms/step - loss: 0.8581 - categorical_accuracy: 0.6315 - val_loss: 0.7598 - val_categorical_accuracy: 0.6812\n",
            "Epoch 2/20\n",
            "58/57 [==============================] - 24s 415ms/step - loss: 0.6398 - categorical_accuracy: 0.7231 - val_loss: 0.7116 - val_categorical_accuracy: 0.6917\n",
            "Epoch 3/20\n",
            "58/57 [==============================] - 24s 411ms/step - loss: 0.5062 - categorical_accuracy: 0.7915 - val_loss: 0.9775 - val_categorical_accuracy: 0.6583\n",
            "Epoch 4/20\n",
            "58/57 [==============================] - 24s 406ms/step - loss: 0.3980 - categorical_accuracy: 0.8244 - val_loss: 0.9159 - val_categorical_accuracy: 0.6583\n",
            "Epoch 5/20\n",
            "58/57 [==============================] - 23s 405ms/step - loss: 0.3026 - categorical_accuracy: 0.8691 - val_loss: 0.9426 - val_categorical_accuracy: 0.6479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZVPNUQcuyAU3"
      },
      "source": [
        "##Evaluating on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmTBFfmfVaql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e5f1a3ed-b4aa-4ae1-82f2-5844511d79c5"
      },
      "source": [
        "test_loss, test_categorical_accuracy = model2.evaluate([test_x, test_aspect], test_y, batch_size=batch_size)\n",
        "print(f\"test loss is {test_loss}, categorical test accuracy is {test_categorical_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "638/638 [==============================] - 3s 4ms/step\n",
            "test loss is 1.0172974099186147, categorical test accuracy is 0.6708333492279053\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}